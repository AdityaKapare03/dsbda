{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27c47565",
      "metadata": {
        "id": "27c47565"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "boston = pd.read_csv('housing_data.csv')\n",
        "boston.head()\n",
        "\n",
        "features = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "boston.shape\n",
        "\n",
        "boston.isnull().sum()\n",
        "\n",
        "boston[features]=boston[features].fillna(boston[features].median())\n",
        "boston.isnull().sum()\n",
        "\n",
        "plt.figure(figsize=(9,9))\n",
        "sns.heatmap(data=boston.corr().round(2), annot=True, square=True, cmap='coolwarm', linewidth=0.2)\n",
        "\n",
        "prime_features = ['LSTAT', 'RM', 'DIS', 'NOX', 'TAX', 'MEDV']\n",
        "\n",
        "df = boston[['LSTAT', 'RM', 'DIS', 'NOX', 'TAX', 'MEDV']]\n",
        "df.shape\n",
        "\n",
        "sns.pairplot(df)\n",
        "\n",
        "for feature in prime_features:\n",
        "    plt.figure(figsize=(6,3))\n",
        "    plt.subplot(121)\n",
        "    plt.title(f'{feature} Distribution')\n",
        "    sns.histplot(df[feature], kde=True)\n",
        "    plt.subplot(122)\n",
        "    plt.title(f'{feature} Boxplot')\n",
        "    sns.boxplot(df[feature])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def outliers(data, threshold = 1.5):\n",
        "    q1 = data.quantile(0.25)\n",
        "    q3 = data.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower = q1 - iqr*threshold\n",
        "    upper = q3 + iqr*threshold\n",
        "    return (data<lower)|(data>upper)\n",
        "\n",
        "count = df.apply(lambda x: outliers(x).sum())\n",
        "print(count)\n",
        "\n",
        "outlier_cols = ['LSTAT', 'MEDV', 'RM']\n",
        "outlier_mask = df[outlier_cols].apply(lambda x: outliers(x)).any(axis=1)\n",
        "df = df[~outlier_mask]\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L4S1IiqvR8Q4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4S1IiqvR8Q4",
        "outputId": "eea450d1-517f-4b81-c9e4-a3a0f3c643a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Values imputed successfully\n"
          ]
        }
      ],
      "source": [
        "tax_10=df2[(df2['TAX']<600)&(df2['LSTAT']>=0)&(df2['LSTAT']<10)]['TAX'].mean()\n",
        "tax_20=df2[(df2['TAX']<600)&(df2['LSTAT']>=10)&(df2['LSTAT']<20)]['TAX'].mean()\n",
        "tax_30=df2[(df2['TAX']<600)&(df2['LSTAT']>=20)&(df2['LSTAT']<30)]['TAX'].mean()\n",
        "tax_40=df2[(df2['TAX']<600)&(df2['LSTAT']>=30)]['TAX'].mean()\n",
        "\n",
        "indexes = list(df2.index)\n",
        "for i in indexes:\n",
        "    if df2['TAX'][i] > 600:\n",
        "        if (0 <= df2['LSTAT'][i] < 10):\n",
        "            df2.at[i,'TAX'] = tax_10\n",
        "        elif (10 <= df2['LSTAT'][i] < 20):\n",
        "            df2.at[i,'TAX'] = tax_20\n",
        "        elif (20 <= df2['LSTAT'][i] < 30):\n",
        "            df2.at[i,'TAX'] = tax_30\n",
        "        elif (df2['LSTAT'][i] >30):\n",
        "            df2.at[i,'TAX'] = tax_40\n",
        "\n",
        "print('Values imputed successfully')\n",
        "\n",
        "x=df3.iloc[:,0:4].values\n",
        "y=df3.iloc[:,-1:].values\n",
        "\n",
        "print(f\"Shape of dependent variable{x.shape}\")\n",
        "print(f\"Shape of independent variable{y.shape}\")\n",
        "\n",
        "def feature_scaling(x):\n",
        "  mean=np.mean(x,axis=0)\n",
        "  std=np.std(x,axis=0)\n",
        "\n",
        "  for i in range(x.shape[1]):\n",
        "    x[:,i]=(x[:,i]-mean[i])/std[i]\n",
        "  return x\n",
        "\n",
        "x = feature_scaling(x)\n",
        "\n",
        "m,n = x.shape\n",
        "\n",
        "x = np.append(arr=np.ones((m,1)),values=x,axis=1)\n",
        "\n",
        "#Now we will spit our data into Train set and Test Set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state = 42)\n",
        "\n",
        "print(f\"Shape of X_train = {X_train.shape}\")\n",
        "print(f\"Shape of X_test = {X_test.shape}\")\n",
        "print(f\"Shape of y_train = {y_train.shape}\")\n",
        "print(f\"Shape of y_test = {y_test.shape}\")\n",
        "\n",
        "def ComputeCost(X,y,theta):\n",
        "    m=X.shape[0] #number of data points in the set\n",
        "    J = (1/(2*m)) * np.sum((X.dot(theta) - y)**2)\n",
        "    return J\n",
        "\n",
        "def GradientDescent(X,y,theta,alpha,no_of_iters):\n",
        "    \"\"\"\n",
        "    Gradient Descent Algorithm to minimize the Cost\n",
        "\n",
        "    Input <- X, y and theta are numpy arrays\n",
        "            X -> Independent Variables/ Features\n",
        "            y -> Dependent/ Target Variable\n",
        "            theta -> Parameters\n",
        "            alpha -> Learning Rate i.e. size of each steps we take\n",
        "            no_of_iters -> Number of iterations we want to perform\n",
        "\n",
        "    Return -> theta (numpy array) which are the best parameters for our dataset to fit a linear line\n",
        "             and Cost Computed (numpy array) for each iteration\n",
        "    \"\"\"\n",
        "    m=X.shape[0]\n",
        "    J_Cost = []\n",
        "    for i in range(no_of_iters):\n",
        "        error = np.dot(X.transpose(),(X.dot(theta)-y))\n",
        "        theta = theta - alpha * (1/m) * error\n",
        "        J_Cost.append(ComputeCost(X,y,theta))\n",
        "\n",
        "    return theta, np.array(J_Cost)\n",
        "\n",
        "def ComputeCost(X,y,theta):\n",
        "    m=X.shape[0] #number of data points in the set\n",
        "    J = (1/(2*m)) * np.sum((X.dot(theta) - y)**2)\n",
        "    return J\n",
        "\n",
        "def GradientDescent(X,y,theta,alpha,no_of_iters):\n",
        "    \"\"\"\n",
        "    Gradient Descent Algorithm to minimize the Cost\n",
        "\n",
        "    Input <- X, y and theta are numpy arrays\n",
        "            X -> Independent Variables/ Features\n",
        "            y -> Dependent/ Target Variable\n",
        "            theta -> Parameters\n",
        "            alpha -> Learning Rate i.e. size of each steps we take\n",
        "            no_of_iters -> Number of iterations we want to perform\n",
        "\n",
        "    Return -> theta (numpy array) which are the best parameters for our dataset to fit a linear line\n",
        "             and Cost Computed (numpy array) for each iteration\n",
        "    \"\"\"\n",
        "    m=X.shape[0]\n",
        "    J_Cost = []\n",
        "    for i in range(no_of_iters):\n",
        "        error = np.dot(X.transpose(),(X.dot(theta)-y))\n",
        "        theta = theta - alpha * (1/m) * error\n",
        "        J_Cost.append(ComputeCost(X,y,theta))\n",
        "\n",
        "    return theta, np.array(J_Cost)\n",
        "\n",
        "iters = 1000\n",
        "\n",
        "alpha1 = 0.001\n",
        "theta1 = np.zeros((X_train.shape[1],1))\n",
        "theta1, J_Costs1 = GradientDescent(X_train,y_train,theta1,alpha1,iters)\n",
        "\n",
        "alpha2 = 0.003\n",
        "theta2 = np.zeros((X_train.shape[1],1))\n",
        "theta2, J_Costs2 = GradientDescent(X_train,y_train,theta2,alpha2,iters)\n",
        "\n",
        "alpha3 = 0.01\n",
        "theta3 = np.zeros((X_train.shape[1],1))\n",
        "theta3, J_Costs3 = GradientDescent(X_train,y_train,theta3,alpha3,iters)\n",
        "\n",
        "alpha4 = 0.03\n",
        "theta4 = np.zeros((X_train.shape[1],1))\n",
        "theta4, J_Costs4 = GradientDescent(X_train,y_train,theta4,alpha4,iters)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(J_Costs1,label = 'alpha = 0.001')\n",
        "plt.plot(J_Costs2,label = 'alpha = 0.003')\n",
        "plt.plot(J_Costs3,label = 'alpha = 0.01')\n",
        "plt.plot(J_Costs4,label = 'alpha = 0.03')\n",
        "plt.title('Convergence of Gradient Descent for different values of alpha')\n",
        "plt.xlabel('No. of iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "theta4\n",
        "\n",
        "def Predict(X,theta):\n",
        "    \"\"\"\n",
        "    This function predicts the result for the unseen data\n",
        "    \"\"\"\n",
        "    y_pred = X.dot(theta)\n",
        "    return y_pred\n",
        "\n",
        "y_pred = Predict(X_test,theta4)\n",
        "y_pred[:5]\n",
        "\n",
        "plt.scatter(x=y_test,y=y_pred,alpha=0.5)\n",
        "plt.xlabel('y_test',size=12)\n",
        "plt.ylabel('y_pred',size=12)\n",
        "plt.title('Predicited Values vs Original Values (Test Set)',size=15)\n",
        "plt.show()\n",
        "\n",
        "sns.residplot(x=y_pred,y=(y_pred-y_test))\n",
        "plt.xlabel('Predicited Values',size=12)\n",
        "plt.ylabel(\"Residues\",size=12)\n",
        "plt.title('Residual Plot',size=15)\n",
        "plt.show()\n",
        "\n",
        "sns.distplot(y_pred-y_test)\n",
        "plt.xlabel('Residual',size=12)\n",
        "plt.ylabel('Frquency',size=12)\n",
        "plt.title('Distribution of Residuals',size=15)\n",
        "plt.show()\n",
        "\n",
        "from sklearn import metrics\n",
        "r2= metrics.r2_score(y_test,y_pred)\n",
        "N,p = X_test.shape\n",
        "adj_r2 = 1-((1-r2)*(N-1))/(N-p-1)\n",
        "print(f'R^2 = {r2}')\n",
        "print(f'Adjusted R^2 = {adj_r2}')\n",
        "\n",
        "from sklearn import metrics\n",
        "mse = metrics.mean_squared_error(y_test,y_pred)\n",
        "mae = metrics.mean_absolute_error(y_test,y_pred)\n",
        "rmse = np.sqrt(metrics.mean_squared_error(y_test,y_pred))\n",
        "print(f'Mean Squared Error: {mse}',f'Mean Absolute Error: {mae}',f'Root Mean Squared Error: {rmse}',sep='\\n')\n",
        "\n",
        "plt.scatter(x=y_test,y=y_pred,alpha=0.5)\n",
        "plt.xlabel('y_test',size=12)\n",
        "plt.ylabel('y_pred',size=12)\n",
        "plt.title('Predicited Values vs Original Values (Test Set)',size=15)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "sns.residplot(x=y_pred,y=(y_pred-y_test))\n",
        "plt.xlabel('Predicited Values',size=12)\n",
        "plt.ylabel(\"Residues\",size=12)\n",
        "plt.title('Residual Plot',size=15)\n",
        "plt.show()\n",
        "\n",
        "sns.distplot(y_pred-y_test)\n",
        "plt.xlabel('Residual',size=12)\n",
        "plt.ylabel('Frquency',size=12)\n",
        "plt.title('Distribution of Residuals',size=15)\n",
        "plt.show()\n",
        "\n",
        "from sklearn import metrics\n",
        "r2= metrics.r2_score(y_test,y_pred)\n",
        "N,p = X_test.shape\n",
        "adj_r2 = 1-((1-r2)*(N-1))/(N-p-1)\n",
        "print(f'R^2 = {r2}')\n",
        "print(f'Adjusted R^2 = {adj_r2}')\n",
        "\n",
        "from sklearn import metrics\n",
        "mse = metrics.mean_squared_error(y_test,y_pred)\n",
        "mae = metrics.mean_absolute_error(y_test,y_pred)\n",
        "rmse = np.sqrt(metrics.mean_squared_error(y_test,y_pred))\n",
        "print(f'Mean Squared Error: {mse}',f'Mean Absolute Error: {mae}',f'Root Mean Squared Error: {rmse}',sep='\\n')\n",
        "\n",
        "#coefficients of regression model\n",
        "coeff=np.array([y for x in theta4 for y in x]).round(2)\n",
        "features=['Bias','RM','TAX','PTRATIO','LSTAT']\n",
        "eqn = 'MEDV = '\n",
        "for f,c in zip(features,coeff):\n",
        "    eqn+=f\" + ({c} * {f})\";\n",
        "\n",
        "print(eqn)\n",
        "\n",
        "\n",
        "sns.barplot(x=features,y=coeff)\n",
        "plt.ylim([-5,25])\n",
        "plt.xlabel('Coefficient Names',size=12)\n",
        "plt.ylabel('Coefficient Values',size=12)\n",
        "plt.title('Visualising Regression Coefficients',size=15)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18pePduyHbH-",
      "metadata": {
        "id": "18pePduyHbH-"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
